{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe65e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b20291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 118 files: 100%|██████████| 118/118 [18:53<00:00,  9.61s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Specify the mission you want to download.\n",
    "mission = \"2024-10-01-11-47-44\"\n",
    "\n",
    "# Download the full dataset\n",
    "#allow_patterns = [f\"*\"]\n",
    "\n",
    "# Download all data from a single mission\n",
    "allow_patterns = [f\"{mission}/*\"]\n",
    "\n",
    "# Download a specific topic\n",
    "#topic = \"alphasense_front_center\"\n",
    "#allow_patterns = [f\"{mission}/*{topic}*\", f\"{mission}/*.yaml\"]\n",
    "\n",
    "\n",
    "# If this is interuppted during download, simply re-run the block and huggingface_hub will resume the download without re-downloading the already downloaded files.\n",
    "hugging_face_data_cache_path = snapshot_download(repo_id=\"leggedrobotics/grand_tour_dataset\", allow_patterns=allow_patterns, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9336639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\rohan\\\\.cache\\\\huggingface\\\\hub\\\\datasets--leggedrobotics--grand_tour_dataset\\\\snapshots\\\\8b9e7951a70081e04edfaf9434f809c7d53d2964'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_face_data_cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e63b5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be extracted to: C:\\Users\\rohan\\grand_tour_dataset\\missions\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the destination directory\n",
    "dataset_folder = Path(\"~/grand_tour_dataset/missions\").expanduser()\n",
    "dataset_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print for confirmation\n",
    "print(f\"Data will be extracted to: {dataset_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98d566a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\grand_tour_dataset\\missions\n",
      "Moved data from C:\\Users\\rohan\\.cache\\huggingface\\hub\\datasets--leggedrobotics--grand_tour_dataset\\snapshots\\8b9e7951a70081e04edfaf9434f809c7d53d2964 to C:\\Users\\rohan\\grand_tour_dataset\\missions!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import fnmatch\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "def move_dataset(cache, dataset_folder, allow_patterns=(\"*\",)):\n",
    "    \"\"\"\n",
    "    Copy/extract a HF snapshot into dataset_folder:\n",
    "      - Matches files using allow_patterns on RELATIVE POSIX paths (works on Windows)\n",
    "      - Resolves Windows `.symlink` stubs to real blob targets\n",
    "      - Extracts real `.tar` archives into the parent of their destination path\n",
    "      - Copies other files (e.g., .zgroup, .zattrs), dropping the `.symlink` suffix\n",
    "    \"\"\"\n",
    "    cache = Path(cache)\n",
    "    dataset_folder = Path(dataset_folder)\n",
    "    dataset_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def allowed(rel_posix: str) -> bool:\n",
    "        return any(fnmatch.fnmatch(rel_posix, pat) for pat in allow_patterns)\n",
    "\n",
    "    def resolve_hf_symlink(p: Path) -> Path:\n",
    "        \"\"\"\n",
    "        On Windows, HF writes tiny text files with `.symlink` extension.\n",
    "        The file content is the absolute path to the real blob/file.\n",
    "        \"\"\"\n",
    "        # Some anti-virus tools can lock files; retry lightly if needed.\n",
    "        target_text = p.read_text(encoding=\"utf-8\").strip()\n",
    "        target = Path(target_text)\n",
    "        if not target.is_absolute():\n",
    "            target = (p.parent / target).resolve()\n",
    "        return target\n",
    "\n",
    "    # 1) List all files in the snapshot\n",
    "    candidates = [p for p in cache.rglob(\"*\") if p.is_file()]\n",
    "    # 2) Filter using relative POSIX paths against allow_patterns\n",
    "    files = []\n",
    "    for p in candidates:\n",
    "        rel_posix = p.relative_to(cache).as_posix()\n",
    "        if allowed(rel_posix):\n",
    "            files.append(p)\n",
    "\n",
    "    # 3) Walk the selected files and either extract (if tar) or copy\n",
    "    for src in files:\n",
    "        rel = src.relative_to(cache)\n",
    "\n",
    "        # If this is a .symlink, resolve to the real blob target\n",
    "        is_symlink_stub = (src.suffix == \".symlink\")\n",
    "        if is_symlink_stub:\n",
    "            real = resolve_hf_symlink(src)\n",
    "            # Where should it land? use the same relative path but drop the .symlink suffix\n",
    "            dst_path = (dataset_folder / rel).with_suffix(\"\")  # strip \".symlink\"\n",
    "            real_suffix = real.suffix.lower()\n",
    "        else:\n",
    "            real = src\n",
    "            dst_path = dataset_folder / rel\n",
    "            real_suffix = real.suffix.lower()\n",
    "\n",
    "        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 3a) Tar? -> extract into the parent folder of the destination\n",
    "        if real_suffix == \".tar\":\n",
    "            try:\n",
    "                with tarfile.open(real, mode=\"r:\") as tf:  # plain .tar (not gz)\n",
    "                    tf.extractall(path=dst_path.parent)\n",
    "            except tarfile.ReadError as e:\n",
    "                print(f\"[WARN] Bad tar: {real} :: {e}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"[WARN] Missing blob for: {src}\")\n",
    "        else:\n",
    "            # 3b) Not a tar -> copy (this includes .zgroup, .zattrs, YAML, etc.)\n",
    "            # If the real target doesn't exist (stale symlink), warn and continue\n",
    "            if not real.exists():\n",
    "                print(f\"[WARN] Missing blob for: {src}\")\n",
    "                continue\n",
    "            try:\n",
    "                shutil.copy2(real, dst_path)\n",
    "            except PermissionError as e:\n",
    "                # Occasional Windows lock issues\n",
    "                print(f\"[WARN] Permission error copying {real} -> {dst_path}: {e}\")\n",
    "\n",
    "    print(f\"Moved data from {cache} to {dataset_folder}!\")\n",
    "print(dataset_folder)\n",
    "move_dataset(hugging_face_data_cache_path, dataset_folder, allow_patterns=allow_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7527ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
